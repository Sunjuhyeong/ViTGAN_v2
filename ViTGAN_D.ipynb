{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViTGAN_D",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72148c76959c4681b875edb16e7ab98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_931a054b2b484d069b7f583456cab7b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b880a708c4d74b5d9aaa4aee07c7168d",
              "IPY_MODEL_4425a94d85164a15ab584ca0795e0bbc",
              "IPY_MODEL_4b9ebccd0032418ba3bbf19b0d858ed0"
            ]
          }
        },
        "931a054b2b484d069b7f583456cab7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b880a708c4d74b5d9aaa4aee07c7168d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_89396d5444f1461b935b3bffd8b6ad15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_532a9aa1f1aa43a1ae61f1a8b7c2ce11"
          }
        },
        "4425a94d85164a15ab584ca0795e0bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_228cf815b2cd4cb68032ab7955babf9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1da2306dfb034ba29e1a663fe1181ae3"
          }
        },
        "4b9ebccd0032418ba3bbf19b0d858ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f208d52ff8bd4552a7aff09ea9b8620a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:05&lt;00:00, 31369549.61it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_429f8f051daf4237956365cc916de82f"
          }
        },
        "89396d5444f1461b935b3bffd8b6ad15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "532a9aa1f1aa43a1ae61f1a8b7c2ce11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "228cf815b2cd4cb68032ab7955babf9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1da2306dfb034ba29e1a663fe1181ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f208d52ff8bd4552a7aff09ea9b8620a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "429f8f051daf4237956365cc916de82f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunjuhyeong/ViTGAN_v2/blob/main/ViTGAN_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-00ItEsd-_"
      },
      "source": [
        "Todo & Questions\n",
        "---------------------\n",
        "CheckList\n",
        "\n",
        "1.   SN, G, D 떼고 확인\n",
        "\n",
        "    -> G_simple, D_simple에 들어가는 Batch Norm의  weight init std가 0.02가 아니라 1로 설정되어있어서 값이 너무 컸다. \n",
        "\n",
        "    -> 원래 G, D를 사용할 때는 StyleGAN 논문에 따라 값을 1 로 한다.\n",
        "\n",
        "    -> 코드에서 쓰이는 batch_size=B 이지만 b_size라는 값이 다르게 있다. 이를 잘못 사용했었다. 주의하기! \n",
        "\n",
        "2.  G 만 붙이고 확인\n",
        "\n",
        "    -> CUDA out of memory라서 Batch size와 Number of Hidden layer를 줄였다.\n",
        "    이게 영향이 있을까?\n",
        "    \n",
        "    -> modulated INR 가능하면 병렬처리 하기\n",
        "\n",
        "    ->  Generator가 학습이 안된다. 해결중\n",
        "\n",
        "    -> modulate가 안되는 것 같다. -> p.data를 붙여서 해결\n",
        "\n",
        "2. (2) G\n",
        "\n",
        "    -> Backpropagation 잘 되는지 확인\n",
        "    -> G 중에서도 ablation 할 수 있는지 확인\n",
        "    -> init 떼기 ( 큰 영향 없나 확인. 요즘은 optimizer 기법 등이 발달해서 초기화가 크게 중요하지 않다고 하셨다.)\n",
        "\n",
        "\n",
        "3.  D만 붙이고 확인\n",
        "\n",
        "\n",
        "4.  ISN 작동시키기\n",
        "\n",
        "\n",
        "\n",
        "Todo\n",
        "\n",
        "*   ISN (dcgan에 spectral norm 붙여보기)\n",
        "*   기초적인 GAN 모델 가져와서 masking 하면서, 틀린 부분 찾아보기 \n",
        "\n",
        "Question\n",
        "\n",
        "1.   Spectral normalization 에서, 값이 30정도로 엄청 큰데 이러면 weight가 매번 너무 커지는 게 아닌가? 내가 initial을 이상한 곳에서 계산하고 있나?\n",
        "2.   linear layer weight에 초기 spetral norm 값을 곱하는 함수 자체도 working하지 않은 것 같다.\n",
        "\n",
        "\n",
        "  (1) with no_grad?\n",
        "  \n",
        "  (2) weight.data? weight? \n",
        "  \n",
        "  (3) .copy? \n",
        "  \n",
        "  (4) net.state_dict? \n",
        "  \n",
        "\n",
        "3.   \n",
        "\n",
        "\n",
        "Memo\n",
        "*      train dataset에는 50000의 이미지가 들어있고, 이를 10개의 batch로 나누면 len(dataloader) = 5000이다. 또한 128의 batch로 나누면 len(dataloader) = 391 이다.\n",
        "\n",
        "\n",
        "*   어째서 줘도 변화가 없을까? (multiply_initial_spectral_norm 함수 참고) \n",
        "      \n",
        "    -> initial_spectral_norm을 dict로 바꿔보자.\n",
        "\n",
        "\n",
        "*   Loss를 보고, 이게 acceptable한 Loss인지 어떻게 알 수 있을까?\n",
        "      \n",
        "    -> GAN은 Loss가 우상향 또는 우하향하면 안된다. 내시 균형을 향해 가야하기 때문이다. 따라서 Loss로 학습이 잘 되는 지 판단하기는 어렵고, FID score를 도입하거나 qualitative evaluation을 할 수 밖에 없다.\n",
        "\n",
        "\n",
        "*   D * L 에 맞추려면 D, L 차원을 펼쳐야 하나? 맞게 펼쳤나?\n",
        "    \n",
        "    -> 보통은 L이 들어가지 않는다. 왜냐하면 patch별로 다르게 MLP, Attention을 적용하는 게 아니기 때문에, L 차원을 따로 고려할 필요 없다. 항상 D를 받아 D를 내는 게 encoder. (vit-pytorch 참고) \n",
        "\n",
        "*   Discriminator classification layer에서 값이 0~1로 나오려면 sigmoid를 추가해야하나?\n",
        "    \n",
        "    -> sigmoid는 쓰지 않은 상태로 BCEWithLogitsLoss를 사용하기\n",
        "    -> 번외로, multi-class 일 때는 logit 별로 softmax를 사용해야 한다. \n",
        "\n",
        "*   왜 Optimizer가 아닌 netD를 직접 zeroGrad 하나? \n",
        "\n",
        "    -> optimizer에 등록된 weight만 업데이트하냐, 혹은 전체 netD를 업데이트 하냐의 차이이지만, 이 상황에서는 차이가 없다. \n",
        "    -> 만약 하나의 network에 여러가지 optimizer가 붙어있다면, 이때는 model.zero_grad()를 호출하는 게 좋다.\n",
        "\n",
        "*   초기 weight의 Spectral norm (Largest singular value) 계산하는 방법 있나?   \n",
        "    \n",
        "    -> spectral norm soruce 코드에서 찾기\n",
        "\n",
        "\n",
        "*   Training 부분에서 출력값들의 의미\n",
        "    \n",
        "    -> D(x)는 real_label(=1.)에, D_1(G(z)) 은 fake_label(=0.)에 가까워야 한다. Discriminator 학습할 때의 값이다.\n",
        "    \n",
        "    -> D_2(G(z))은 real_label(=1.)에 가까워야 한다. \n",
        "    Generator 학습할 때의 값이다.\n",
        "\n",
        "    -> Discriminator출력에 Sigmoid를 씌우고 생각하면 된다. D(x) 등에 음수가 나와도 이상한 것이 아니다. 단 Loss는 맞게 계산되어야 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlTtUTFAZfi3"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5gxEmO5w-5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e3a5ac-4356-46df-922c-beb01032a7e6"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-cWGVsalaJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1467bd6d-8dc5-4764-e721-bc3cd7c41b1f"
      },
      "source": [
        "!pip install easydict\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2aC9oqoZfjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64824c5d-a8c9-4d35-a3be-eed63b2d7775"
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data \n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from einops import rearrange, repeat\n",
        "from easydict import EasyDict as edict\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff3fe652c90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNm51HguZfjC"
      },
      "source": [
        "Inputs\n",
        "------\n",
        "\n",
        "Let’s define some inputs for the run:\n",
        "\n",
        "-  **dataroot** - the path to the root of the dataset folder. We will\n",
        "   talk more about the dataset in the next section\n",
        "-  **workers** - the number of worker threads for loading the data with\n",
        "   the DataLoader\n",
        "-  **batch_size** - the batch size used in training. The DCGAN paper\n",
        "   uses a batch size of 128\n",
        "-  **image_size** - the spatial size of the images used for training.\n",
        "   This implementation defaults to 64x64. If another size is desired,\n",
        "   the structures of D and G must be changed. See\n",
        "   `here <https://github.com/pytorch/examples/issues/70>`__ for more\n",
        "   details\n",
        "-  **nc** - number of color channels in the input images. For color\n",
        "   images this is 3\n",
        "-  **nz** - length of latent vector\n",
        "-  **ngf** - relates to the depth of feature maps carried through the\n",
        "   generator\n",
        "-  **ndf** - sets the depth of feature maps propagated through the\n",
        "   discriminator\n",
        "-  **num_epochs** - number of training epochs to run. Training for\n",
        "   longer will probably lead to better results but will also take much\n",
        "   longer\n",
        "-  **lr** - learning rate for training. As described in the DCGAN paper,\n",
        "   this number should be 0.0002\n",
        "-  **beta1** - beta1 hyperparameter for Adam optimizers. As described in\n",
        "   paper, this number should be 0.5\n",
        "-  **ngpu** - number of GPUs available. If this is 0, code will run in\n",
        "   CPU mode. If this number is greater than 0 it will run on that number\n",
        "   of GPUs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRLyYgWZfjD"
      },
      "source": [
        "H = W = 32\n",
        "\n",
        "patch_h = patch_w = 4\n",
        "\n",
        "overlap_h = patch_h // 2\n",
        "overlap_w = patch_w // 2\n",
        "\n",
        "nph = H // patch_h\n",
        "\n",
        "npw = W // patch_w \n",
        "\n",
        "B =  10 # 128 in paper\n",
        "\n",
        "L = nph * npw  \n",
        "\n",
        "# Root directory for dataset\n",
        "dataroot = \"data/CIFAR10\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = B\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "#image_size = 64\n",
        "image_size = 32\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of w latent vector (i.e. size of generator input)\n",
        "nw = 100\n",
        "\n",
        "# Number of Head of Attention Layer\n",
        "num_heads = 6\n",
        "\n",
        "# embedding dimension\n",
        "D = 384\n",
        "\n",
        "# Size of hidden dimension in MLP\n",
        "n_hidden = 300 # 1536 in paper\n",
        "\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "# num_epochs = 5\n",
        "num_epochs = 1\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.0\n",
        "beta2 = 0.99\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1\n",
        "\n",
        "# depth\n",
        "depth = L // 8\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "# print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS53b6nh03pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e61e40-ee0b-4977-8832-3dd20c66d6ee"
      },
      "source": [
        "h_basis = torch.linspace(0, nph-1, nph).div(nph-1).mul(2).sub(1)\n",
        "w_basis = torch.linspace(0, npw-1, npw).div(npw-1).mul(2).sub(1)\n",
        "coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "coords2d = torch.stack(coords2d, 0).reshape(2, nph*npw).t() \n",
        "# positional_embedding_patch_position = coords2d.unsqueeze(0).repeat(B, 1, 1)\n",
        "positional_embedding_patch_position = coords2d\n",
        "positional_embedding_patch_position = positional_embedding_patch_position.to(device)\n",
        "print(positional_embedding_patch_position.shape) # [patch_positions_in_an_image, D(=2)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn9Ze-SP-KUi"
      },
      "source": [
        "# # original Fourier embedding\n",
        "\n",
        "# h_basis = torch.linspace(0, patch_h-1, patch_h).div(patch_h-1).mul(2).sub(1)\n",
        "# w_basis = torch.linspace(0, patch_w-1, patch_w).div(patch_w-1).mul(2).sub(1)\n",
        "# coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "# coords2d = torch.stack(coords2d, 0).reshape(2, patch_h*patch_w).t()\n",
        "# coords2d = coords2d.unsqueeze(0).repeat(B, 1, 1)\n",
        "# positional_embedding_patch_pixel = coords2d.unsqueeze(1).repeat(1, L, 1, 1)\n",
        "# positional_embedding_patch_pixel = positional_embedding_patch_pixel.to(device)\n",
        "# print(positional_embedding_patch_pixel.shape) #[B, number_of_patch, h*w, D(=2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDT_ozejOvpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85668421-31ce-4262-9b6b-2c29354aada5"
      },
      "source": [
        "h_basis = torch.linspace(0, patch_h-1, patch_h).div(patch_h-1).mul(2).sub(1)\n",
        "w_basis = torch.linspace(0, patch_w-1, patch_w).div(patch_w-1).mul(2).sub(1)\n",
        "coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "coords2d = torch.stack(coords2d, 0).reshape(2, patch_h*patch_w).t()\n",
        "positional_embedding_patch_pixel = coords2d.to(device)\n",
        "print(positional_embedding_patch_pixel.shape) #[h*w, nc(=2)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNKLROTyZfjE"
      },
      "source": [
        "Data\n",
        "----\n",
        "\n",
        "In this tutorial we will use the `Celeb-A Faces\n",
        "dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__ which can\n",
        "be downloaded at the linked site, or in `Google\n",
        "Drive <https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`__.\n",
        "The dataset will download as a file named *img_align_celeba.zip*. Once\n",
        "downloaded, create a directory named *celeba* and extract the zip file\n",
        "into that directory. Then, set the *dataroot* input for this notebook to\n",
        "the *celeba* directory you just created. The resulting directory\n",
        "structure should be:\n",
        "\n",
        "::\n",
        "\n",
        "   /path/to/celeba\n",
        "       -> img_align_celeba  \n",
        "           -> 188242.jpg\n",
        "           -> 173822.jpg\n",
        "           -> 284702.jpg\n",
        "           -> 537394.jpg\n",
        "              ...\n",
        "\n",
        "This is an important step because we will be using the ImageFolder\n",
        "dataset class, which requires there to be subdirectories in the\n",
        "dataset’s root folder. Now, we can create the dataset, create the\n",
        "dataloader, set the device to run on, and finally visualize some of the\n",
        "training data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIYU1lT2ZfjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "72148c76959c4681b875edb16e7ab98d",
            "931a054b2b484d069b7f583456cab7b7",
            "b880a708c4d74b5d9aaa4aee07c7168d",
            "4425a94d85164a15ab584ca0795e0bbc",
            "4b9ebccd0032418ba3bbf19b0d858ed0",
            "89396d5444f1461b935b3bffd8b6ad15",
            "532a9aa1f1aa43a1ae61f1a8b7c2ce11",
            "228cf815b2cd4cb68032ab7955babf9b",
            "1da2306dfb034ba29e1a663fe1181ae3",
            "f208d52ff8bd4552a7aff09ea9b8620a",
            "429f8f051daf4237956365cc916de82f"
          ]
        },
        "outputId": "4d55c4c5-0478-4023-8e5d-7504d3f31486"
      },
      "source": [
        "# # We can use an image folder dataset the way we have it setup.\n",
        "# # Create the dataset\n",
        "# dataset = dset.ImageFolder(root=dataroot,\n",
        "#                            transform=transforms.Compose([\n",
        "#                                transforms.Resize(image_size),\n",
        "#                                transforms.CenterCrop(image_size),\n",
        "#                                transforms.ToTensor(),\n",
        "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                            ]))\n",
        "# # Create the dataloader\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "#                                          shuffle=True, num_workers=workers)\n",
        "\n",
        "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
        "training_data = dset.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
        "test_data = dset.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# 데이터로더를 생성합니다.\n",
        "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break\n",
        "\n",
        "dataloader = train_dataloader\n",
        "\n",
        "# Plot some training images\n",
        "# real_batch = next(iter(train_dataloader))\n",
        "# plt.figure(figsize=(8,8))\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Training Images\")\n",
        "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72148c76959c4681b875edb16e7ab98d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Shape of X [N, C, H, W]:  torch.Size([10, 3, 32, 32])\n",
            "Shape of y:  torch.Size([10]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJfQGbOb7diQ"
      },
      "source": [
        "class PixelNormLayer(nn.Module):\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_W6GmCTZfjF"
      },
      "source": [
        "# Generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf9PqtO0ZfjG"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "# 초기화 방법이 나와있지 않다.\n",
        "# styleGan에선 N(0,1) 을 따르게 했다.\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        print(classname)\n",
        "        nn.init.normal_(m.weight.data, 0.0, (0.1 * (2 ** 0.5)))    \n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jtRsFUMUsuJ"
      },
      "source": [
        "class SLN(nn.Module):\n",
        "  def __init__(self, w):\n",
        "        super(SLN, self).__init__()\n",
        "        self.w = w\n",
        "        self.gamma = nn.Linear(in_features=nw, out_features=D, device=device)\n",
        "        self.beta = nn.Linear(in_features=nw, out_features=D, device=device) \n",
        "\n",
        "  def forward(self, input):\n",
        "        # input is positional embedding of patch position\n",
        "        mean = torch.mean(input, dim=-1, keepdim=True)\n",
        "        variance = torch.var(input, dim=-1, keepdim=True)\n",
        "        # print(\"input of SLN\", input.size()) # L D\n",
        "        self.w = self.w.unsqueeze(1) # B 1 nz\n",
        "        gamma = self.gamma(self.w) # B 1 D\n",
        "        beta = self.beta(self.w) # B 1 D\n",
        "\n",
        "        # print(\"input: \\n\", input.size())\n",
        "        # print(\"gamma: \\n\", gamma.size())\n",
        "        # print(\"beta: \\n\", beta.size())\n",
        "        # print(\"mean: \\n\", mean.size())\n",
        "        # print(\"variance: \\n\", variance.size())\n",
        "        output = gamma * ((input - mean) / variance) + beta\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QSjXxmgxDiP"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "          nn.Linear(in_features=in_features, out_features=n_hidden, bias=True, device=device),\n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=out_features, bias=True, device=device),    \n",
        "          # nn.LeakyReLU(negative_slope=0.01)\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "        output = self.mlp(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG8L6LVPWyd_"
      },
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "  def __init__(self, in_features, out_features, n_hidden):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "\n",
        "        self.pixel_norm = PixelNormLayer()\n",
        "        self.mapping_network = nn.Sequential(\n",
        "          nn.Linear(in_features=in_features, out_features=n_hidden, bias=True),\n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=out_features, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01)\n",
        "        )\n",
        "        \n",
        "        # -------Code for check the value of layer parameters & z -----------\n",
        "        # self.linear1 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.linear2 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.linear3 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.linear4 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        # -------------------------------------------------------------------\n",
        "\n",
        "  def forward(self, input):\n",
        "        input = self.pixel_norm(input)\n",
        "\n",
        "        # -------Code for check the value of layer parameters & z -----------\n",
        "        # input = self.linear1(input)\n",
        "        # input = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # print(\"linear 1\", self.linear1.weight)\n",
        "        # input = self.linear2(input)\n",
        "        # input = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # input = self.linear3(input)\n",
        "        # input = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # input = self.linear4(input)\n",
        "        # output = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # -------------------------------------------------------------------\n",
        "        output = self.mapping_network(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DauDkC8xaXuO"
      },
      "source": [
        "class GeneratorEncoderBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, w):\n",
        "        super(GeneratorEncoderBlock, self).__init__()\n",
        "        self.w = w\n",
        "        self.sln1 = SLN(self.w)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, device=device)\n",
        "        self.sln2 = SLN(self.w)\n",
        "        self.mlp = MLP(in_features=embed_dim, out_features=embed_dim).to(device)\n",
        "\n",
        "  def forward(self, input):            \n",
        "        # input is positional embedding of patch position\n",
        "        # print(\"input\",input.size())\n",
        "        temp = self.sln1(input)\n",
        "        # print(\"sln1\",temp.size())\n",
        "\n",
        "        temp = self.attention.forward(query=temp, key=temp, value=temp)\n",
        "        # print(\"attention\",temp[0].size())\n",
        "        temp = temp[0] + input\n",
        "        output = self.sln2(temp)\n",
        "        # print(\"sln2\",temp.size())\n",
        "        output = self.mlp(output)\n",
        "        # print(\"mlp\",output.size())\n",
        "        output = output + temp\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDaupxPHdLJg"
      },
      "source": [
        "class GeneratorEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, depth, w):\n",
        "        super(GeneratorEncoder, self).__init__()\n",
        "        self.encoder_blocks = []\n",
        "        for i in range(depth):\n",
        "          self.encoder_blocks.append(GeneratorEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, w=w))\n",
        "        self.sln = SLN(w=w)\n",
        "\n",
        "  def forward(self, input):\n",
        "        output = input\n",
        "        for i in range(depth):\n",
        "          output = self.encoder_blocks[i](output)\n",
        "        output = self.sln(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwU47cnEeX92"
      },
      "source": [
        "class PositionalEmbeddingLayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(PositionalEmbeddingLayer, self).__init__()\n",
        "        self.positional_embedding_layer = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "  def forward(self, input):\n",
        "        output = self.positional_embedding_layer(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7MqEr5_h9x9"
      },
      "source": [
        "class FourierEmbeddingLayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(FourierEmbeddingLayer, self).__init__()\n",
        "        # self.fourier_embedding_layer = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        self.fourier_embedding_layer = nn.Parameter(torch.randn([in_features, out_features], device=device), requires_grad=True)\n",
        "  def forward(self, input):\n",
        "        # print(\"fourier input\", input)\n",
        "        output = input @ self.fourier_embedding_layer # 16, 2 -> 16, D\n",
        "        # print(\"fourier output\", output)\n",
        "        output = torch.sin(output)\n",
        "        # print(\"fourier sine output\", output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gUignvDll9R"
      },
      "source": [
        "INR\n",
        "----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0mB7aH_kN77"
      },
      "source": [
        "class ImplicitNeuralNetwork(nn.Module):\n",
        "  def __init__(self, embed_dim, n_hidden):\n",
        "        super(ImplicitNeuralNetwork, self).__init__()\n",
        "        self.y = y\n",
        "        self.linear1 = nn.Linear(in_features=embed_dim, out_features=n_hidden, device=device)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.02)\n",
        "        self.linear2 = nn.Linear(in_features=n_hidden, out_features=nc, device=device) \n",
        "        self.lrelu2 = nn.LeakyReLU(0.02)\n",
        "        self.implicit_neural_network = nn.Sequential(\n",
        "          self.linear1,\n",
        "          self.lrelu1,\n",
        "          self.linear2,\n",
        "          # self.lrelu2\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "        # print(\"input size\", input.size())\n",
        "        # output = self.implicit_neural_network(input)\n",
        "        # print(\"input of INR\", input)\n",
        "        input = self.linear1(input)\n",
        "        # print(\"linear1 weight\", self.linear1.weight.data.size(), self.linear1.weight.data)\n",
        "        input = self.lrelu1(input)\n",
        "        # print(\"after linear 1\", input)\n",
        "        output = self.linear2(input)\n",
        "        # output = self.lrelu2(input)\n",
        "        # print(\"linear2 weight\", self.linear2.weight.data.size(), self.linear2.weight.data)\n",
        "        # print(\"after linear 2\", output)\n",
        "        # print(\"output size\", output.size())\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thVbRCgllqel"
      },
      "source": [
        "def modulate_inr (s1, s2, inr, epsilon):\n",
        "  for name, p in inr.named_parameters():\n",
        "    if name == 'linear1.weight':\n",
        "      # s1 = repeat(s1, 'B L D -> B L n_hidden D', B=B, L=L, D=D, n_hidden=n_hidden)\n",
        "      # s1 = repeat(s1, 'L D -> L n_hidden D', L=L, D=D, n_hidden=n_hidden)\n",
        "      s1 = repeat(s1, 'D -> n_hidden D', D=D, n_hidden=n_hidden)\n",
        "      # print(\"p size\", p.size())\n",
        "      # print(\"s1 size\", s1.size())\n",
        "      temp = torch.sum(s1*s1*p.data*p.data, dim=-1)\n",
        "      # print(\"temp\", temp)\n",
        "      # print(\"temp size\", temp.size())\n",
        "      # temp = repeat(temp, 'B L n_hidden -> B L  n_hidden D', B=B, L=L, D=D, n_hidden=n_hidden)\n",
        "      # temp = repeat(temp, 'L n_hidden -> L n_hidden D', L=L, D=D, n_hidden=n_hidden)\n",
        "      temp = repeat(temp, ' n_hidden -> n_hidden D', D=D, n_hidden=n_hidden)\n",
        "      # print(\"p\",p)\n",
        "      # print(\"s1\", s1)\n",
        "      # print(\"dividor\", torch.sqrt(epsilon + temp))\n",
        "      p.data = p.data * s1 / torch.sqrt(epsilon + temp)   \n",
        "      # print(\"p after\",p)   \n",
        "    elif  name == 'linear2.weight': \n",
        "      # s2 = repeat(s2, 'B L n_hidden -> B L nc n_hidden', B=B, L=L, nc=nc, n_hidden=n_hidden)\n",
        "      # s2 = repeat(s2, 'L n_hidden -> L nc n_hidden', L=L, nc=nc, n_hidden=n_hidden)\n",
        "      s2 = repeat(s2, 'n_hidden -> nc n_hidden', nc=nc, n_hidden=n_hidden)\n",
        "      temp = torch.sum(s2*s2*p.data*p.data, dim=-1)\n",
        "      # temp = repeat(temp, 'B L nc -> B L nc n_hidden', B=B, L=L, nc=nc, n_hidden=n_hidden)\n",
        "      # temp = repeat(temp,  'L nc -> L nc n_hidden', L=L, nc=nc, n_hidden=n_hidden)\n",
        "      temp = repeat(temp,  'nc -> nc n_hidden', nc=nc, n_hidden=n_hidden)\n",
        "      p.data = p.data * s2 / torch.sqrt(epsilon + temp)\n",
        "  return inr  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp0aS0yKlIe7"
      },
      "source": [
        "class ModulatedINR(nn.Module):\n",
        "  def __init__(self, embed_dim, n_hidden, patch_h, patch_w, y):\n",
        "        super(ModulatedINR, self).__init__()\n",
        "        # modulated MLP\n",
        "        self.y = y\n",
        "        self.epsilon = 1e-8\n",
        "        self.patch_h = patch_h\n",
        "        self.patch_w = patch_w\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_hidden = n_hidden\n",
        "        self.linear1 = nn.Linear(embed_dim, embed_dim, device=device)\n",
        "        self.linear2 = nn.Linear(embed_dim, n_hidden, device=device)\n",
        "        self.inr = ImplicitNeuralNetwork(embed_dim=embed_dim, n_hidden=n_hidden)\n",
        "        self.modulated_inr = copy.deepcopy(self.inr).to(device)\n",
        "        self.output = torch.zeros([B, L, (patch_h*patch_w), nc], device=device)\n",
        "\n",
        "  def forward(self, input): \n",
        "        # print(\"y\", self.y)\n",
        "        s1 = self.linear1(self.y) # D to D\n",
        "        # print(\"s1\", s1)\n",
        "\n",
        "        s2 = self.linear2(self.y) # D to n_hidden\n",
        "        # print(\"s2\", s2)\n",
        "\n",
        "        # print(s1.size()) # B L D\n",
        "        # print(s2.size()) # B L n_hidden\n",
        "        \n",
        "        # 일단 for loop으로 하고 나중에 병렬로 바꿔보기\n",
        "        for i in range(B): # B\n",
        "          for j in range(L): # L\n",
        "            self.modulated_inr = copy.deepcopy(self.inr).to(device)\n",
        "            new_modulated_inr = modulate_inr(s1[i][j], s2[i][j], self.modulated_inr, self.epsilon)\n",
        "            self.output[i][j] = new_modulated_inr(input)\n",
        "        # print(\"output\", self.output.size(), self.output)\n",
        "        return self.output     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofPyfVWlYxc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be482459-302a-439b-b4cb-725b86ee344e"
      },
      "source": [
        "a = torch.zeros([1])\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0380PAP__fKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcb248f-a943-4c04-bfc8-33e7b42f53cf"
      },
      "source": [
        "class Test(nn.Module):\n",
        "  def __init__(self, y):\n",
        "      super(Test, self).__init__()\n",
        "      self.y = y\n",
        "      self.linear = nn.Linear(1, 1)\n",
        "      self.linear2 = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.linear(self.y) # B L D\n",
        "    print(x.size())\n",
        "    y = self.linear2(input) # npwnph D\n",
        "    return x\n",
        "\n",
        "y = torch.ones([3,2,1]) # B L D\n",
        "a = Test(y)\n",
        "input = torch.ones([4, 1]) # B L npwnph D\n",
        "a(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0238],\n",
              "         [-0.0238]],\n",
              "\n",
              "        [[-0.0238],\n",
              "         [-0.0238]],\n",
              "\n",
              "        [[-0.0238],\n",
              "         [-0.0238]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSi83DUyZfjH"
      },
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.mapping_network = MappingNetwork(in_features=nz, out_features=nw, n_hidden=nz)\n",
        "        self.fourier_embedding_layer = FourierEmbeddingLayer(in_features=2, out_features=D)\n",
        "        self.positional_embedding_layer = PositionalEmbeddingLayer(in_features=2, out_features=D)\n",
        "        self.inr_list = []\n",
        "        self.output_patches = []\n",
        "\n",
        "    def forward(self, z):\n",
        "        # print(\"Z\", z)\n",
        "        w = self.mapping_network(z)\n",
        "        # print(\"w\", w)\n",
        "        generator_encoder = GeneratorEncoder(embed_dim=D, num_heads=num_heads, depth=depth, w=w)\n",
        "        positional_embedding = self.positional_embedding_layer(positional_embedding_patch_position) # [L, D]\n",
        "        y = generator_encoder(positional_embedding) \n",
        "        # print(\"y, y size\", y, y.size()) # [B, L, D]\n",
        "        fourier_embedding = self.fourier_embedding_layer(positional_embedding_patch_pixel) # [(patch_h patch_w), D]\n",
        "        # patch = torch.zeros([L, patch_h*patch_w, nc], device=device, requires_grad=False) # requires_grad=False가 맞을까? \n",
        "        # print(\"fourier_embedding , size \", fourier_embedding, fourier_embedding.size()) # [B, L, D]\n",
        "    \n",
        "        self.inr_list = ModulatedINR(embed_dim=D, n_hidden=n_hidden, patch_h=patch_h, patch_w=patch_w, y=y) \n",
        "        patch = self.inr_list(fourier_embedding) # phpw D\n",
        "        # print(\"patch\", patch, patch.size()) # [B, L, D]\n",
        "        \n",
        "        output = rearrange(patch, 'B (nph npw) (patch_h patch_w) nc -> B nc (nph patch_h) (npw patch_w)', B=B, nph=nph, npw=npw, patch_h=patch_h, patch_w=patch_w, nc=nc) # 맞게 rearrange 한걸까?\n",
        "        # B nc H W\n",
        "        # print(\"output after rearrange\", output, output.size()) # [B, L, D]\n",
        "        # output = rearrange(patch, '(nph npw) (patch_h patch_w) nc -> nc (nph patch_h) (npw patch_w)', nph=nph, npw=npw, patch_h=patch_h, patch_w=patch_w, nc=nc) # 맞게 rearrange 한걸까?\n",
        "        output = torch.sigmoid(output)\n",
        "        # print(\"output after sigmoid\", patch, patch.size()) # [B, L, D]\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRuGISepZfjI"
      },
      "source": [
        "NetG\n",
        "---------------\n",
        "\n",
        ", we can instantiate the generator and apply the ``weights_init``\n",
        "function. Check out the printed model to see how the generator object is\n",
        "structured.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLvg3sLLEwrK"
      },
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator_Simple(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator_Simple, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            # nn.BatchNorm2d(nc),\n",
        "            # nn.ReLU(True),\n",
        "            nn.Tanh()\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            # nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        # print(\"output of generator\", output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiuVpVegZfjI"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Create the generator\n",
        "# netG = Generator(ngpu).to(device)\n",
        "netG = Generator_Simple(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "# netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "# print(netG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzhMUNU6Ck5V"
      },
      "source": [
        "# ## Train with all-fake batch\n",
        "# # Generate batch of latent vectors\n",
        "# noise = torch.randn(B, nz, device=device) \n",
        "# # noise = torch.randn(b_size, nz, 1, 1, device=device)  # Noise for simple network\n",
        "# # Generate fake image batch with G\n",
        "# fake = netG(noise)\n",
        "# # print(fake.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHN0EbRHTuMP"
      },
      "source": [
        "# Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-DE5In3aayn"
      },
      "source": [
        "class ProjectingPatches(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(ProjectingPatches, self).__init__()\n",
        "        self.project_layer = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "\n",
        "  def forward(self, input):\n",
        "        output = self.project_layer(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkpQMHfVoz55"
      },
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn=None):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        if self.fn != None:\n",
        "          return self.fn(self.norm(x), **kwargs)\n",
        "        else:\n",
        "          return self.norm(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFeZAJNCPEll"
      },
      "source": [
        "# ##### Transformer\n",
        "\n",
        "# args = edict()\n",
        "# MAX_LEN = 100\n",
        "\n",
        "# class NewMultiheadAttention(nn.Module):\n",
        "#     \"\"\"\n",
        "#     A vanilla multi-head masked attention layer with a projection at the end.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, embed_dim, num_heads, mask=False):\n",
        "#         super(NewMultiheadAttention, self).__init__()\n",
        "#         # mask : whether to use \n",
        "#         # key, query, value projections for all heads\n",
        "#         args.nhid_tran = embed_dim\n",
        "#         args.nhead = num_heads\n",
        "#         self.key = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "#         self.query = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "#         self.value = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "#         # output projection\n",
        "#         self.proj = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "#         # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "#         if mask:\n",
        "#             self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN)))\n",
        "#         self.nhead = args.nhead\n",
        "\n",
        "#     def forward(self, q, k, v, mask=None):\n",
        "#         B = q.size(0)\n",
        "#         T_q = q.size(1)\n",
        "#         T = k.size(1)\n",
        "#         nhead = self.nhead\n",
        "#         hidden_size =  args.nhid_tran\n",
        "#         d_k = hidden_size // nhead\n",
        "#         q = self.query(q)\n",
        "#         k = self.key(k)\n",
        "#         v = self.value(v)\n",
        "#         q = q.view(B, T_q, nhead, d_k)\n",
        "#         k = k.view(B, T, nhead, d_k)\n",
        "#         v = k.view(B, T, nhead, d_k)\n",
        "#         q = torch.transpose(q, 1, 2)\n",
        "#         k = torch.transpose(k, 1, 2)\n",
        "#         v = torch.transpose(v, 1, 2)\n",
        "        \n",
        "#         temp = torch.cdist(q.detach().clone(), k.detach().clone()) #(B,nhead,T_q,T) /  vectorized Euclidean L2 distances\n",
        "#         temp = temp / (d_k ** 0.5)\n",
        "#         if hasattr(self, 'mask'):\n",
        "#           mask2 = self.mask[:T_q, :T].to(torch.bool)\n",
        "#           mask2 = torch.unsqueeze(mask2, 0).expand(nhead, T_q, T)\n",
        "#           mask2 = torch.unsqueeze(mask2, 0).expand(B, nhead, T_q, T)\n",
        "#           temp = temp.masked_fill(mask2==0, float('-inf'))\n",
        "#         if mask != None:\n",
        "#           mask = torch.unsqueeze(mask, 1).expand(B, T_q, T)\n",
        "#           mask = torch.unsqueeze(mask, 1).expand(B, nhead, T_q, T)\n",
        "#           temp = temp.masked_fill(mask==0, float('-inf'))\n",
        "#         temp = torch.nn.functional.softmax(temp, dim=-1)\n",
        "#         outputs = torch.matmul(temp, v)\n",
        "#         outputs = torch.transpose(outputs, 1, 2)\n",
        "#         outputs = torch.reshape(outputs, (B, T_q, hidden_size))\n",
        "#         outputs = self.proj(outputs)\n",
        "#         return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cid-LA-Qxl8J"
      },
      "source": [
        "##### Transformer\n",
        "\n",
        "args = edict()\n",
        "MAX_LEN = 100\n",
        "\n",
        "class NewMultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked attention layer with a projection at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, device, mask=False):\n",
        "        super(NewMultiheadAttention, self).__init__()\n",
        "        # mask : whether to use \n",
        "        # key, query, value projections for all heads\n",
        "        args.nhid_tran = embed_dim\n",
        "        args.nhead = num_heads\n",
        "        self.key = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        self.query = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        self.value = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        if mask:\n",
        "            self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN, device=device)))\n",
        "        self.nhead = args.nhead\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B = q.size(0)\n",
        "        T_q = q.size(1)\n",
        "        T = k.size(1)\n",
        "        nhead = self.nhead\n",
        "        hidden_size =  args.nhid_tran\n",
        "        d_k = hidden_size // nhead\n",
        "        q = self.query(q)\n",
        "        k = self.key(k)\n",
        "        v = self.value(v)\n",
        "        q = q.view(B, T_q, nhead, d_k)\n",
        "        k = k.view(B, T, nhead, d_k)\n",
        "        v = k.view(B, T, nhead, d_k)\n",
        "        q = torch.transpose(q, 1, 2)\n",
        "        k = torch.transpose(k, 1, 2)\n",
        "        v = torch.transpose(v, 1, 2)\n",
        "        \n",
        "        temp = torch.cdist(q.detach().clone(), k.detach().clone()) #(B,nhead,T_q,T) /  vectorized Euclidean L2 distances\n",
        "        temp = temp / (d_k ** 0.5)\n",
        "        if hasattr(self, 'mask'):\n",
        "          mask2 = self.mask[:T_q, :T].to(torch.bool)\n",
        "          mask2 = torch.unsqueeze(mask2, 0).expand(nhead, T_q, T)\n",
        "          mask2 = torch.unsqueeze(mask2, 0).expand(B, nhead, T_q, T)\n",
        "          temp = temp.masked_fill(mask2==0, float('-inf'))\n",
        "        if mask != None:\n",
        "          mask = torch.unsqueeze(mask, 1).expand(B, T_q, T)\n",
        "          mask = torch.unsqueeze(mask, 1).expand(B, nhead, T_q, T)\n",
        "          temp = temp.masked_fill(mask==0, float('-inf'))\n",
        "        temp = torch.nn.functional.softmax(temp, dim=-1)\n",
        "        outputs = torch.matmul(temp, v)\n",
        "        outputs = torch.transpose(outputs, 1, 2)\n",
        "        outputs = torch.reshape(outputs, (B, T_q, hidden_size))\n",
        "        outputs = self.proj(outputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkA_jTRfZ8JG"
      },
      "source": [
        "# class DiscriminatorEncoderBlock(nn.Module):\n",
        "#   def __init__(self, embed_dim):\n",
        "#         super(DiscriminatorEncoderBlock, self).__init__()\n",
        "#         self.ln1 = nn.LayerNorm(normalized_shape=D) \n",
        "#         self.attention = NewMultiheadAttention(embed_dim=embed_dim, num_heads=num_heads).to(device)\n",
        "#         self.ln2 = nn.LayerNorm(normalized_shape=D) \n",
        "#         self.mlp = MLP(in_features=embed_dim, out_features=embed_dim).to(device)\n",
        "\n",
        "#   def forward(self, input):                \n",
        "#         temp = self.ln1(input)\n",
        "#         temp = self.attention.forward(temp, temp, temp)\n",
        "#         temp = temp + input\n",
        "#         output = self.ln2(temp)\n",
        "#         output = self.mlp(output)\n",
        "#         output = output + temp\n",
        "#         return output        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw4cizhwW5Eq"
      },
      "source": [
        "# class DiscriminatorEncoder(nn.Module):\n",
        "#   def __init__(self):\n",
        "#         super(DiscriminatorEncoder, self).__init__()\n",
        "#         self.encoder_blocks = []\n",
        "#         for i in range(depth):\n",
        "#           self.encoder_blocks.append(DiscriminatorEncoderBlock(embed_dim=D).to(device))\n",
        "\n",
        "#   def forward(self, input):\n",
        "#         output = input\n",
        "#         for i in range(depth):\n",
        "#           output = self.encoder_blocks[i](output)\n",
        "#         return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCh_P36NxO9R"
      },
      "source": [
        "class DiscriminatorEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, depth):\n",
        "        super(DiscriminatorEncoder, self).__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "          self.encoder_blocks.append(\n",
        "              nn.ModuleList([\n",
        "                PreNorm(embed_dim),\n",
        "                NewMultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, device=device),\n",
        "                PreNorm(embed_dim, MLP(in_features=embed_dim, out_features=embed_dim))\n",
        "            ])\n",
        "          )\n",
        "\n",
        "  def forward(self, x):\n",
        "        for sln, attn, ff in self.encoder_blocks:\n",
        "            temp = sln(x)\n",
        "            temp = attn(temp, temp, temp) \n",
        "            x = temp + x\n",
        "            x = ff(x) + x  \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iG_m8GSvSXM"
      },
      "source": [
        "def overlap_input(img):\n",
        "  # img = torch.zeros([B, nc, H, W]) -> 맞춰주기!\n",
        "  padding = (overlap_w, overlap_w, overlap_h, overlap_h)\n",
        "  img = nn.functional.pad(img, padding)\n",
        "  stride_h = patch_h\n",
        "  stride_w = patch_w\n",
        "  img_patches = img.unfold(2, patch_h+2*overlap_h, stride_h).unfold(3, patch_w+2*overlap_w, stride_w) \n",
        "  # [B, nc, nph, npw, p_h+2*o_h, p_w+2*o_w]\n",
        "  return img_patches\n",
        "\n",
        "# img = torch.zeros([B, nc, H, W]) \n",
        "# overlap_input(img).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3RZHtvjZfjK"
      },
      "source": [
        "# class Discriminator(nn.Module):\n",
        "#     def __init__(self, ngpu):\n",
        "#         super(Discriminator, self).__init__()\n",
        "#         self.ngpu = ngpu\n",
        "\n",
        "#         self.class_embedding = torch.zeros([1, 1, D], dtype=torch.float, requires_grad=True, device=device)\n",
        "#         self.position_zero = torch.zeros([1, 1, D], dtype=torch.float, requires_grad=True, device=device)\n",
        "#         self.class_embedding = repeat(self.class_embedding, '1 1 D -> B 1 D', B=B, D=D)\n",
        "#         self.position_zero = repeat(self.position_zero, '1 1 D -> B 1 D', B=B, D=D)\n",
        "#         # self.projecting_patches = ProjectingPatches(in_features=(patch_h + 2*overlap_h) * (patch_w + 2*overlap_w) * nc, out_features=D).to(device)\n",
        "#         self.projecting_patches = ProjectingPatches(in_features=(patch_h) * (patch_w) * nc, out_features=D).to(device)\n",
        "#         self.positional_embedding_layer = PositionalEmbeddingLayer(in_features=2, out_features=D).to(device)\n",
        "#         self.encoder = DiscriminatorEncoder().to(device)\n",
        "#         self.output_layer = nn.Linear(in_features=D, out_features=1, device=device)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         # input = rearrange(input, 'B H W nc -> B nc H W', B=B, nc=nc, H=H, W=W)\n",
        "#         # print(\"input\", input)\n",
        "#         # patches = overlap_input(input) # [B, nc, nph, npw, p_h+2*o_h, p_w+2*o_w]\n",
        "#         patches = input.to(device) # [B, nc, nph, npw, p_h+2*o_h, p_w+2*o_w]\n",
        "#         # print(\"patches\", patches)\n",
        "        \n",
        "#         # for patch in patches:\n",
        "#         # flatten_patches = rearrange(patches, 'B nc nph npw patch_h patch_w -> B (nph npw) (patch_h patch_w nc)', B=patches.size(0), nph=nph, npw=npw, patch_h=patch_h+2*overlap_h, patch_w=patch_w+2*overlap_w, nc=nc)\n",
        "#         flatten_patches = rearrange(patches, 'B nc (nph patch_h) (npw patch_w) -> B (nph npw) (patch_h patch_w nc)', B=patches.size(0), nph=nph, npw=npw, patch_h=patch_h, patch_w=patch_w, nc=nc)\n",
        "        \n",
        "#         # print(\"flatten_patches\", flatten_patches)\n",
        "#         projected_flatten_patches = self.projecting_patches(flatten_patches) # [B, L, h*w*nc] -> [B, L, D]\n",
        "#         # print(\"projected_flatten_patches\", projected_flatten_patches)\n",
        "#         patch_embedding = torch.cat([self.class_embedding, projected_flatten_patches], dim=1) # [B, L, D] -> [B, L+1, D] \n",
        "#         # print(\"patch_embedding\", patch_embedding)\n",
        "        \n",
        "#         positional_embedding = self.positional_embedding_layer(positional_embedding_patch_position) # [B, L+1, D] 가 되어야하는데\n",
        "#         positional_embedding = repeat(positional_embedding, 'L D -> B L D', B=B, L=L, D=D)\n",
        "#         # print(\"positional_embedding\", positional_embedding)\n",
        "#         positional_embedding = torch.cat([self.position_zero, positional_embedding], dim=1) # self.position_zero 맞게 init 했나? dim=1이 맞나?\n",
        "#         # print(\"positional_embedding\", positional_embedding)\n",
        "        \n",
        "#         tokens = positional_embedding + patch_embedding\n",
        "#         # print(\"tokens\", tokens)\n",
        "#         y = self.encoder(tokens) # [B, L+1, D]\n",
        "#         # print(\"y\", y)\n",
        "#         index = torch.tensor([0]).to(device)\n",
        "#         output_token = torch.index_select(y, dim=1, index=index) # [B, 1, D]\n",
        "#         # print(\"output_token\", output_token)\n",
        "#         output = self.output_layer(output_token) # [B, 1, 1]\n",
        "#         # print(\"output in discriminator\", output)\n",
        "#         # output = torch.sigmoid(output)\n",
        "#         # print(\"output in discriminator after sigmoid\", output)\n",
        "#         return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN5NZrXdxWNh"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "\n",
        "        self.class_embedding = torch.zeros([1, 1, D], dtype=torch.float, requires_grad=True, device=device)\n",
        "        self.position_zero = torch.zeros([1, 1, D], dtype=torch.float, requires_grad=True, device=device)\n",
        "        self.class_embedding = repeat(self.class_embedding, '1 1 D -> B 1 D', B=B, D=D)\n",
        "        self.position_zero = repeat(self.position_zero, '1 1 D -> B 1 D', B=B, D=D)\n",
        "        self.projecting_patches = ProjectingPatches(in_features=(patch_h + 2*overlap_h) * (patch_w + 2*overlap_w) * nc, out_features=D).to(device)\n",
        "        self.positional_embedding_layer = PositionalEmbeddingLayer(in_features=2, out_features=D).to(device)\n",
        "        self.encoder = DiscriminatorEncoder(D, num_heads, depth).to(device)\n",
        "        self.output_layer = nn.Linear(in_features=D, out_features=1, device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input = rearrange(input, 'B H W nc -> B nc H W', B=B, nc=nc, H=H, W=W)\n",
        "        # print(\"input\", input)\n",
        "        patches = overlap_input(input) # [B, nc, nph, npw, p_h+2*o_h, p_w+2*o_w]\n",
        "        # print(\"patches\", patches)\n",
        "        \n",
        "        # for patch in patches:\n",
        "        flatten_patches = rearrange(patches, 'B nc nph npw patch_h patch_w -> B (nph npw) (patch_h patch_w nc)', B=patches.size(0), nph=nph, npw=npw, patch_h=patch_h+2*overlap_h, patch_w=patch_w+2*overlap_w, nc=nc)\n",
        "        # print(\"flatten_patches\", flatten_patches)\n",
        "        projected_flatten_patches = self.projecting_patches(flatten_patches) # [B, L, h*w*nc] -> [B, L, D]\n",
        "        # print(\"projected_flatten_patches\", projected_flatten_patches)\n",
        "        patch_embedding = torch.cat([self.class_embedding, projected_flatten_patches], dim=1) # [B, L, D] -> [B, L+1, D] \n",
        "        # print(\"patch_embedding\", patch_embedding)\n",
        "        \n",
        "        positional_embedding = self.positional_embedding_layer(positional_embedding_patch_position) # [B, L+1, D] 가 되어야하는데\n",
        "        positional_embedding = repeat(positional_embedding, 'L D -> B L D', B=B, L=L, D=D)\n",
        "        # print(\"positional_embedding\", positional_embedding)\n",
        "        positional_embedding = torch.cat([self.position_zero, positional_embedding], dim=1) # self.position_zero 맞게 init 했나? dim=1이 맞나?\n",
        "        # print(\"positional_embedding\", positional_embedding)\n",
        "        \n",
        "        tokens = positional_embedding + patch_embedding\n",
        "        # print(\"tokens\", tokens.size())\n",
        "        y = self.encoder(tokens) # [B, L+1, D]\n",
        "        # print(\"y\", y)\n",
        "        index = torch.tensor([0]).to(device)\n",
        "        print(\"y size\", y.size())\n",
        "        output_token = torch.index_select(y, dim=1, index=index) # [B, 1, D]\n",
        "        # print(\"output_token\", output_token)\n",
        "        output = self.output_layer(output_token) # [B, 1, 1]\n",
        "        # print(\"output in discriminator\", output)\n",
        "        # output = torch.sigmoid(output)\n",
        "        # print(\"output in discriminator after sigmoid\", output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pbkzm1oNiqL"
      },
      "source": [
        "Spectral Normalization \n",
        "-----------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF_hJwZFyLDb"
      },
      "source": [
        "initial_spectral_norm = {}\n",
        "\n",
        "def spectral_normalization(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    nn.utils.parametrizations.spectral_norm(m)\n",
        "\n",
        "def calculate_initial_spectral_norm(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    global initial_spectral_norm\n",
        "    initial_spectral_norm[m.in_features] = torch.linalg.svdvals(m.weight.data)[0] \n",
        "\n",
        "def multiply_initial_spectral_norm(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    global initial_spectral_norm \n",
        "    print(\"weight before\", m.weight.data)\n",
        "    print(\"multiply this value\", initial_spectral_norm[m.in_features])\n",
        "    # with torch.no_grad():\n",
        "    m.weight.data = torch.mul(m.weight.data , initial_spectral_norm[m.in_features])\n",
        "    print(\"weight after\", m.weight.data)\n",
        "\n",
        "def print_weight(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    print(\"name, weight[0] of linear layer\", m.name, m.weight.data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQMqlC1uEwrN"
      },
      "source": [
        "class Discriminator_Simple(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator_Simple, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # # input is (nc) x 64 x 64\n",
        "            # nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            # nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(input.size())\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-mxs5ZMZfjL"
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "# netD = Discriminator_Simple(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "# netD.apply(weights_init)\n",
        "\n",
        "# Spectral Normalization\n",
        "\n",
        "# netD.apply(calculate_initial_spectral_norm)\n",
        "# netD.apply(spectral_normalization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC7H74H-ZfjL"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, as with the generator, we can create the discriminator, apply the\n",
        "``weights_init`` function, and print the model’s structure.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbj9hskJZfjM"
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(B, nz, 1, 1, device=device)\n",
        "# fixed_noise = torch.randn(64, nz, 1, 1, device=device) # Noise for simple network\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P60ZcFIofDa7"
      },
      "source": [
        "net1 = nn.Linear(10, 20)\n",
        "net2 = nn.Linear(10, 20)\n",
        "\n",
        "params = [\n",
        "    {'params':net1.parameters(), 'lr':0.1},\n",
        "    {'params':net2.parameters(), 'lr':0.01},\n",
        "    {'params':net2.parameters()},\n",
        "]\n",
        "\n",
        "# optimizer(params, lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XMAoW2hOZfjP",
        "outputId": "acb108a0-1089-4b73-97e0-e260f279de20"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "# Setting for debug\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "# torch.autograd.detect_anomaly\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        ###########################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0]\n",
        "        real_cpu = real_cpu.to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        \n",
        "        # Improved Spectral_Normalization\n",
        "        # netD.apply(multiply_initial_spectral_norm)\n",
        "        \n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        \n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device) \n",
        "        # noise = torch.randn(b_size, nz, 1, 1, device=device)  # Noise for simple network\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "\n",
        "        # Improved Spectral_Normalization\n",
        "        # netD.apply(multiply_initial_spectral_norm)\n",
        "\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "\n",
        "        # Improved Spectral_Normalization\n",
        "        # netD.apply(multiply_initial_spectral_norm)\n",
        "\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "[0/1][0/5000]\tLoss_D: 1.5379\tLoss_G: 12.1152\tD(x): 0.2946\tD(G(z)): 0.5103 / -12.1151\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n",
            "y size torch.Size([10, 65, 384])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-497da632c337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Forward pass real batch through D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Calculate loss on all-real batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-1639632c5ee2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_embedding\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpatch_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# print(\"tokens\", tokens.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, L+1, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# print(\"y\", y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-36983bedfff9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-d89d3129dcd2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnhid_tran\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0md_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYS2aTJxZfjR"
      },
      "source": [
        "# Results\n",
        "\n",
        "Finally, lets check out how we did. Here, we will look at three\n",
        "different results. First, we will see how D and G’s losses changed\n",
        "during training. Second, we will visualize G’s output on the fixed_noise\n",
        "batch for every epoch. And third, we will look at a batch of real data\n",
        "next to a batch of fake data from G.\n",
        "\n",
        "**Loss versus training iteration**\n",
        "\n",
        "Below is a plot of D & G’s losses versus training iterations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fHlZrfTZfjS"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nytxGFRZfjT"
      },
      "source": [
        "**Visualization of G’s progression**\n",
        "\n",
        "Remember how we saved the generator’s output on the fixed_noise batch\n",
        "after every epoch of training. Now, we can visualize the training\n",
        "progression of G with an animation. Press the play button to start the\n",
        "animation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLBNraojZfjT"
      },
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD94uqOaZfjT"
      },
      "source": [
        "**Real Images vs. Fake Images**\n",
        "\n",
        "Finally, lets take a look at some real images and fake images side by\n",
        "side.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBFyVIZcZfjU"
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhcDdHWZfjU"
      },
      "source": [
        "# Where to Go Next\n",
        "\n",
        "\n",
        "We have reached the end of our journey, but there are several places you\n",
        "could go from here. You could:\n",
        "\n",
        "-  Train for longer to see how good the results get\n",
        "-  Modify this model to take a different dataset and possibly change the\n",
        "   size of the images and the model architecture\n",
        "-  Check out some other cool GAN projects\n",
        "   `here <https://github.com/nashory/gans-awesome-applications>`__\n",
        "-  Create GANs that generate\n",
        "   `music <https://deepmind.com/blog/wavenet-generative-model-raw-audio/>`__\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA-667Xyw4Wj"
      },
      "source": [
        "# Additional Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm9f-Rg-lo1j"
      },
      "source": [
        "noise = torch.randn(2, 3, 2,2, device=device)\n",
        "noise2 = torch.randn(2, 6, device=device)\n",
        "l = torch.matmul(noise, noise2)\n",
        "\n",
        "\n",
        "# print(l.size())\n",
        "# print(noise.size())\n",
        "\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "noise = torch.randn(3, 2, 1,1, device=device)\n",
        "layer = nn.Linear(1, 6)      \n",
        "l = layer(noise)\n",
        "# print(l.size())\n",
        "\n",
        "# 아래에서 에러가 나는 이유는 matrix multiplication 작동을 몰라서 그렇다.\n",
        "# 차원이 3개 이상인 경우 맨 마지막 두 차원을 제외한 앞의 차원들은 모두 배치 차원이 된다.\n",
        "# 즉, 뒤에 2 차원들 끼리는 matrix multiplication이 되도록 차원을 맞춰야 한다.\n",
        "\n",
        "noise = torch.randn(3, 2, 1,1, device=device)\n",
        "layer = nn.Linear(2, 6)      \n",
        "l = layer(noise)\n",
        "# print(l.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0usTw61w5c9"
      },
      "source": [
        "H = W = 32\n",
        "h = w = 4\n",
        "nh = H // h\n",
        "nw = W // w\n",
        "Ys = torch.randn(1, nh*nw, D)\n",
        "Ys = Ys.unsqueeze(1).repeat(1, 64, 1, 1)\n",
        "Ys.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAFRwJ6ew5c-"
      },
      "source": [
        "H = W = 32\n",
        "h = w = 4\n",
        "nh = H // h\n",
        "nw = W // w\n",
        "B = 128\n",
        "\n",
        "h_basis = torch.linspace(0, nh-1, nh).div(nh-1).mul(2).sub(1)\n",
        "w_basis = torch.linspace(0, nw-1, nw).div(nw-1).mul(2).sub(1)\n",
        "coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "# print(coords2d)\n",
        "\n",
        "coords2d = torch.stack(coords2d, 0).reshape(2, nh*nw).t()\n",
        "coords2d_patch_position = coords2d.unsqueeze(0).repeat(B, 1, 1)\n",
        "# print(coords2d_patch_position.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfprGJsYw5c-"
      },
      "source": [
        "coords_new = rearrange(coords2d, '(nh h nw w) D -> (nh nw) (h w) D', nh=nh, h=h, nw=nw, w=w, D=2)\n",
        "# print(coords_new.unsqueeze(0).repeat(B, 1, 1, 1).shape)\n",
        "\n",
        "coords_new = repeat(coords2d, '(nh h nw w) D -> B (nh nw) (h w) D', B=B, nh=nh, h=h, nw=nw, w=w, D=2).shape # nh h nw w 인지 어떻게 알까?\n",
        "# print(coords_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSp8uTKo-Nqv"
      },
      "source": [
        "이미지를 불러와서 아래와 같이 reshape 해보고, 이미지 그대로 복원되는지 보기.\n",
        " \n",
        "\n",
        "3,64,64\n",
        "\n",
        "4096, 3\n",
        "\n",
        "3, 64, 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxwpTqXjXV4p"
      },
      "source": [
        "D = 256\n",
        "L = 10 # num tokens\n",
        "B = 4 # num batch\n",
        "net = nn.Linear(D, D)\n",
        "\n",
        "input = torch.randn(B, L, D)\n",
        "input.bmm(input.permute(0, 2, 1)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07k-QxKaqMM"
      },
      "source": [
        "class CustomNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Linear(10, 20),\n",
        "        nn.Conv2d(3, 3, 1, 1, 1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.features2 = nn.Sequential(\n",
        "        nn.Linear(10, 20),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.apply(self.__init_weight__)\n",
        "  def forward(self, x):\n",
        "    return None\n",
        "\n",
        "  def __init_weight__(self, m):\n",
        "    if isinstance(m, (nn.Linear,)):\n",
        "      nn.utils.spectral_norm(m)\n",
        "\n",
        "net = CustomNet()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}